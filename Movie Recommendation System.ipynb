{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1957adb1",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e9d66d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-23T03:21:38.306496Z",
     "iopub.status.busy": "2023-04-23T03:21:38.306011Z",
     "iopub.status.idle": "2023-04-23T03:21:39.966458Z",
     "shell.execute_reply": "2023-04-23T03:21:39.965432Z"
    },
    "papermill": {
     "duration": 1.668417,
     "end_time": "2023-04-23T03:21:39.969309",
     "exception": false,
     "start_time": "2023-04-23T03:21:38.300892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from scipy.stats.mstats import winsorize\n",
    "from datetime import datetime\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b19a9c",
   "metadata": {},
   "source": [
    "## Functions - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbcef86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_values(df, col):\n",
    "    \"\"\"\n",
    "    Extracts and flattens values from a dictionary-like structure in a DataFrame column.\n",
    "    :param df: DataFrame containing the column to explode.\n",
    "    :param col: Column name to be processed.\n",
    "    :return: List of flattened values.\n",
    "    \"\"\"\n",
    "    return [name for item_list in df[col].dropna().map(eval).tolist() for item in item_list for name in item['name']]\n",
    "\n",
    "def top_k_replace(df, col, k, val):\n",
    "    \"\"\"\n",
    "    Replaces all but the top k elements in a DataFrame column with a specified value.\n",
    "    :param df: DataFrame containing the column.\n",
    "    :param col: Column name to be processed.\n",
    "    :param k: Number of top elements to keep.\n",
    "    :param val: Value to replace other elements with.\n",
    "    \"\"\"\n",
    "    top = df[col].value_counts().nlargest(k).index\n",
    "    df[col] = df[col].where(df[col].isin(top), other=val)\n",
    "\n",
    "def zero_replace(df, col, val):\n",
    "    \"\"\"\n",
    "    Replaces zero values in a DataFrame column with a specified value.\n",
    "    :param df: DataFrame containing the column.\n",
    "    :param col: Column name where zeros are to be replaced.\n",
    "    :param val: Value to replace zeros with.\n",
    "    \"\"\"\n",
    "    df[col] = df[col].replace(0, val)\n",
    "\n",
    "def extract_key_values_list(df, col):\n",
    "    \"\"\"\n",
    "    Similar to extract_key_values but returns a 2-dimensional list of values.\n",
    "    :param df: DataFrame containing the column to explode.\n",
    "    :param col: Column name to be processed.\n",
    "    :return: 2D list of values.\n",
    "    \"\"\"\n",
    "    return df[col].dropna().map(lambda x: [item['name'] for item in eval(x)]).tolist()\n",
    "\n",
    "# splits columns into left and right columns given delimiter\n",
    "# input: df->dataframe, col->string of column name, lcol->left column name, rcol->right column string name, delim->delimiter \n",
    "# output: updated dataframe with the left and right columns included\n",
    "def column_split(df, col, lcol, rcol, delim):\n",
    "    ls_lcol = []\n",
    "    ls_rcol = []\n",
    "    for i in df[col]:\n",
    "        sep = i.split(delim)\n",
    "        lval = np.int_(sep[0])\n",
    "        rval = np.int_(sep[1])\n",
    "        ls_lcol.append(lval)\n",
    "        ls_rcol.append(rval)\n",
    "    df[lcol] = ls_lcol\n",
    "    df[rcol] = ls_rcol\n",
    "    return df\n",
    "## merges train and metadata into a single dataset, ditto with test\n",
    "def get_datasets(m):\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    train = column_split(train, 'userId_movieId', 'user_id','id', '_')\n",
    "    train = train.merge(m, how = 'left', on = 'id')\n",
    "    train.fillna(False, inplace = True)\n",
    "    \n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "    test = column_split(test, \"userId_movieId\", \"user_id\", \"id\", \"_\")\n",
    "    test = test.merge(m, how = 'left', on = 'id')\n",
    "    test.fillna(False, inplace = True)\n",
    "    return (test, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b746af7",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01850b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text=text.strip()\n",
    "    text=re.compile('<.*?>').sub('', text)\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text\n",
    "# stopword removal\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "\n",
    "## collaborative filtering functions\n",
    "# returns list of once one-hot encoded column names based on given regular expression\n",
    "def get_column_names(df, regexpr):\n",
    "    col_names = []\n",
    "    for col in df.columns:\n",
    "        x = re.findall(regexpr, col)\n",
    "        if x:\n",
    "            col_names.append(x[0])\n",
    "    return col_names\n",
    "\n",
    "# input: df->dataframe, col->name of dataframe, col_names->list of column names\n",
    "# output: dataframe of averages based on col_names\n",
    "# goal: constructs dataframe of name col with columns col_names with data from the mean of the ratings partitioned by col_names\n",
    "def get_ratings(df, col, col_names):\n",
    "    ratings = {}\n",
    "    for col_name in col_names:\n",
    "        avg_rating_name = df[df[col_name] == True]['rating'].mean()\n",
    "        if np.isnan(avg_rating_name):\n",
    "            avg_rating_name = 0 # 0 cuz no data\n",
    "        ratings[col_name] = avg_rating_name\n",
    "    return pd.DataFrame(pd.Series(ratings, name = col)).astype(float)\n",
    "\n",
    "\n",
    "# merges data (again) with the collaborative filtering dataframe\n",
    "def pref_merge(df, pref_df):\n",
    "    df = df.merge(pref_df, how = 'left', on = 'user_id')\n",
    "    df.fillna(False, inplace = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "## modeling functions\n",
    "## evaluates performance\n",
    "def model_evaluate(truth, pred, model_name):\n",
    "    print(\"PERFORMANCE OF {0}\".format(model_name))\n",
    "    rmse = np.sqrt(mean_squared_error(truth, pred))\n",
    "    print(\"RMSE on testing set = \", rmse)\n",
    "    print(\"(mean normalised) RMSE on testing set = \", rmse / truth.mean())\n",
    "    print(\"(minmax normalised) RMSE on testing set = \", rmse / (truth.max() - truth.min()))\n",
    "    print(\"\\n\")\n",
    "    return rmse\n",
    "\n",
    "## runs all the models\n",
    "def run_models(test, train):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        train.drop('rating',axis=1, inplace = False),\n",
    "        train['rating'],\n",
    "        test_size=1/6.0,\n",
    "        random_state=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## 'vanilla' models refer to models with all default hyperparameters\n",
    "    # models with names consisting of just the type of model used refers to models with hyperparameters found through grid search\n",
    "    # 'bayesian' models refer to models with hyperparameters found through bayesian optimisation (or at least close to it)\n",
    "    models = [LinearRegression(),\n",
    "              DecisionTreeRegressor(max_depth = 10, max_leaf_nodes = 64, min_samples_leaf = 4, min_samples_split = 64, \n",
    "                                    splitter = 'best'),\n",
    "              DecisionTreeRegressor(max_depth = 28, max_leaf_nodes = 64, min_samples_leaf = 21, min_samples_split = 96,\n",
    "                                    splitter = 'best'),\n",
    "              RandomForestRegressor(n_estimators = 100, bootstrap = True, max_samples = 1000,\n",
    "                                    max_depth = 10, max_leaf_nodes = 64,\n",
    "                                    min_samples_leaf = 4, min_samples_split = 64),\n",
    "              RandomForestRegressor(n_estimators = 139, bootstrap = True, max_samples = 1906,\n",
    "                                    max_depth = 19, max_leaf_nodes = 95,\n",
    "                                    min_samples_leaf = 3, min_samples_split = 13),\n",
    "              HistGradientBoostingRegressor(),\n",
    "              HistGradientBoostingRegressor(learning_rate = 0.125, max_iter = 100, max_bins = 255,\n",
    "                                            max_leaf_nodes = 63, min_samples_leaf = 20, max_depth = 10),\n",
    "              \n",
    "              # note that this is not exactly optimal, but it gets close.\n",
    "              HistGradientBoostingRegressor(learning_rate = 0.10877288751758378, max_iter = 258, max_bins = 255,\n",
    "                                            max_leaf_nodes = 50, min_samples_leaf = 47, max_depth = 18)\n",
    "             ]\n",
    "    model_names = [\"LINEAR REGRESSION\",\n",
    "                    \"DECISION TREE\",\n",
    "                    \"BAYESIAN DECISION TREE\",\n",
    "                    \"RANDOM FOREST\",\n",
    "                    \"BAYESIAN RANDOM FOREST\",\n",
    "                    \"VANILLA GRADIENT BOOSTING\",\n",
    "                    \"GRADIENT BOOSTING\",\n",
    "                    \"BAYESIAN GRADIENT BOOSTING\"]\n",
    "    \n",
    "    (min_rmse, mindex) = ((2**63) -1, 0)\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        models[i].fit(X_train, y_train)\n",
    "        pred = models[i].predict(X_test)\n",
    "        rmse = model_evaluate(y_test, pred, model_names[i])\n",
    "        if (rmse < min_rmse):\n",
    "            (min_rmse, mindex) = (rmse, i)\n",
    "    print(\"BEST PERFORMING MODEL:\\n\\t{0} WITH RMSE {1}\".format(model_names[mindex], min_rmse))       \n",
    "    return (models[mindex].predict(test), min_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccfc17e",
   "metadata": {},
   "source": [
    "## Content Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee3c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/lbhljqsj1l19mq1wlm29wfnw0000gn/T/ipykernel_59895/3671310756.py:45: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  metadata.fillna(False, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "## clean setup each time\n",
    "metadata = pd.read_csv(\"movies_metadata.csv\")\n",
    "\n",
    "### CLEAN METADATA ###\n",
    "# 1. remove duplicates\n",
    "metadata.drop_duplicates(subset = ['id'], inplace = True)\n",
    "\n",
    "# 2. map removal, top k replace\n",
    "k = 10\n",
    "for col in ['spoken_languages', 'production_companies', 'production_countries']:\n",
    "    metadata[col] = pd.DataFrame(extract_key_values(metadata, col))\n",
    "    misc = 'Other' \n",
    "    top_k_replace(metadata, col, k, misc)\n",
    "misc = 'Foreign'\n",
    "top_k_replace(metadata, 'original_language', k, misc)\n",
    "        \n",
    "# 3. zero replace\n",
    "columns = ['runtime', 'vote_count', 'vote_average']\n",
    "for col in columns:\n",
    "    zero_replace(metadata, col, metadata[col].median() // 1)\n",
    "    \n",
    "# 4. making revenue and budget be based off of thresholds\n",
    "winsorize(metadata['budget'], limits=[0.01, 0.05])\n",
    "winsorize(metadata['revenue'], limits=[0.2875, 0.05])\n",
    "    \n",
    "# 5. one-hot encoding, key-value extraction\n",
    "g = extract_key_values_list(metadata, 'genres')\n",
    "genres = np.unique(np.array(extract_key_values(metadata, 'genres')))\n",
    "for genre in genres:\n",
    "    metadata['is_' + genre] = [genre in l for l in g]\n",
    "col = ['original_language', 'production_companies', 'production_countries', 'spoken_languages']\n",
    "metadata = pd.get_dummies(data = metadata, columns = col)\n",
    "    \n",
    "# 6. updating year\n",
    "# magic string date \"2001-08-03\" is the median date (50th percentile) with untreated NaN\n",
    "dates = pd.DataFrame([datetime.strptime(\"2001-08-03\" if (type(date) != str) else date, \"%Y-%m-%d\") for date in metadata['release_date']])[0]\n",
    "metadata['year'] = pd.DataFrame([date.year for date in dates])[0]\n",
    "metadata['month'] = pd.DataFrame([date.month for date in dates])[0]\n",
    "metadata['decade'] = pd.DataFrame([(date.year // 10) * 10 for date in dates])[0]\n",
    "\n",
    "# 7. make booleans integers\n",
    "metadata.fillna(False, inplace = True)\n",
    "boolies = list(metadata.select_dtypes(bool).columns)\n",
    "metadata[boolies] = metadata[boolies].astype(bool)\n",
    "    \n",
    "# 8. data preprocessing of the 'overview' column\n",
    "text = (metadata['overview'].astype(str)).apply(lambda x: stopword(preprocess(x)))\n",
    "tfidf = TfidfVectorizer(max_features = 20000, ngram_range = (1, 5))\n",
    "\n",
    "# add text data to metadata (now numeric), get cluster labels\n",
    "tfidf.fit(text)\n",
    "kmeans = KMeans(n_clusters = 9, init = 'k-means++', random_state=42, n_init = 10).fit(csr_matrix(hstack([tfidf.transform(text)])))\n",
    "label = kmeans.labels_\n",
    "metadata['label'] = label\n",
    "\n",
    "# 9. columns to be removed\n",
    "to_remove = ['belongs_to_collection', 'homepage', 'imdb_id', 'original_title', 'overview',\n",
    "             'poster_path','status','tagline','title','video', 'release_date', 'genres']\n",
    "metadata.drop(labels = to_remove, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522f446",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21a0affd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/lbhljqsj1l19mq1wlm29wfnw0000gn/T/ipykernel_59895/126728975.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  train.fillna(False, inplace = True)\n",
      "/var/folders/1f/lbhljqsj1l19mq1wlm29wfnw0000gn/T/ipykernel_59895/126728975.py:11: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  test.fillna(False, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "## ratings from all users based on genre and production company\n",
    "(test, train) = get_datasets(metadata)\n",
    "genre_avg = 'genre_averages'\n",
    "production_avg = 'production_averages'\n",
    "genres = get_column_names(train, \"^is_.*\")\n",
    "prefer_genres = [\"prefer_\" + genre[len(\"is_\"):] for genre in genres]\n",
    "productions = get_column_names(train, \"^production_companies_.*\")\n",
    "prefer_productions = [production[len(\"production_companies_\"):] + \"_pref\" for production in productions]\n",
    "\n",
    "## dataframe of genre and production averages across entire dataset\n",
    "genre_avg_rating = get_ratings(train, genre_avg, genres)\n",
    "production_avg_rating = get_ratings(train, production_avg, productions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "290afe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## per user\n",
    "user_genres_pref_df = None\n",
    "user_production_pref_df = None\n",
    "user_ids = np.unique(train['user_id'])\n",
    "\n",
    "## loop to get movie information from movie id, which has different rating\n",
    "for i in user_ids:\n",
    "    df = train[train['user_id'] == i]\n",
    "    user_genre_avg_rating = get_ratings(df, genre_avg, genres)\n",
    "    user_production_avg_rating = get_ratings(df, production_avg, productions)\n",
    "    \n",
    "    # copying over genre row\n",
    "    genre_prefs = pd.Series(\n",
    "    np.where(user_genre_avg_rating[genre_avg] > 0,\n",
    "             user_genre_avg_rating[genre_avg],\n",
    "             genre_avg_rating[genre_avg]), index=prefer_genres\n",
    "    )\n",
    "    genre_prefs_list = pd.DataFrame([genre_prefs for i in range(df.shape[0])])\n",
    "    \n",
    "    # add to new updated dataframe\n",
    "    genre_prefs_dict = {g: genre_prefs_list.get(g, 0) for g in prefer_genres}\n",
    "    genre_df = pd.DataFrame(genre_prefs_dict, index=[0])\n",
    "    genre_df['user_id'] = i\n",
    "    \n",
    "    # add to new dataframe for all users\n",
    "    user_genres_pref_df = pd.concat([user_genres_pref_df, genre_df])\n",
    "\n",
    "    # same with productions\n",
    "    production_prefs = pd.Series(\n",
    "    np.where(user_production_avg_rating[production_avg] > 0,\n",
    "             user_production_avg_rating[production_avg],\n",
    "             production_avg_rating[production_avg]),\n",
    "    index=prefer_productions\n",
    "    )\n",
    "    production_prefs_list = pd.DataFrame([production_prefs for i in range(df.shape[0])])\n",
    "\n",
    "    production_prefs_dict = {p: production_prefs_list.get(p, 0) for p in prefer_productions}\n",
    "    production_df = pd.DataFrame(production_prefs_dict, index=[0])\n",
    "    production_df['user_id'] = i\n",
    "    \n",
    "    # add to new dataframe for all users\n",
    "    user_production_pref_df = pd.concat([user_production_pref_df, production_df])\n",
    "    \n",
    "## merge with the metadata (test/train partitioned)\n",
    "train = pref_merge(pref_merge(train, user_genres_pref_df), user_production_pref_df)\n",
    "test = pref_merge(pref_merge(test, user_genres_pref_df), user_production_pref_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c1b73",
   "metadata": {},
   "source": [
    "## Run all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e508294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE OF LINEAR REGRESSION\n",
      "RMSE on testing set =  0.17826377617103392\n",
      "(mean normalised) RMSE on testing set =  0.25234820992834733\n",
      "(minmax normalised) RMSE on testing set =  0.1980708624122599\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE OF DECISION TREE\n",
      "RMSE on testing set =  0.1789412023265318\n",
      "(mean normalised) RMSE on testing set =  0.2533071668256505\n",
      "(minmax normalised) RMSE on testing set =  0.19882355814059088\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE OF BAYESIAN DECISION TREE\n",
      "RMSE on testing set =  0.17894208030150094\n",
      "(mean normalised) RMSE on testing set =  0.25330840967720786\n",
      "(minmax normalised) RMSE on testing set =  0.19882453366833436\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE OF RANDOM FOREST\n",
      "RMSE on testing set =  0.17783907137567267\n",
      "(mean normalised) RMSE on testing set =  0.2517470025649704\n",
      "(minmax normalised) RMSE on testing set =  0.19759896819519185\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE OF BAYESIAN RANDOM FOREST\n",
      "RMSE on testing set =  0.17612335902628973\n",
      "(mean normalised) RMSE on testing set =  0.2493182593316657\n",
      "(minmax normalised) RMSE on testing set =  0.1956926211403219\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE OF VANILLA GRADIENT BOOSTING\n",
      "RMSE on testing set =  0.17225578134079417\n",
      "(mean normalised) RMSE on testing set =  0.24384335956988107\n",
      "(minmax normalised) RMSE on testing set =  0.1913953126008824\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE OF GRADIENT BOOSTING\n",
      "RMSE on testing set =  0.17110115952634514\n",
      "(mean normalised) RMSE on testing set =  0.24220888982914762\n",
      "(minmax normalised) RMSE on testing set =  0.1901123994737168\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE OF BAYESIAN GRADIENT BOOSTING\n",
      "RMSE on testing set =  0.1697073628206302\n",
      "(mean normalised) RMSE on testing set =  0.24023584678447582\n",
      "(minmax normalised) RMSE on testing set =  0.18856373646736688\n",
      "\n",
      "\n",
      "BEST PERFORMING MODEL:\n",
      "\tBAYESIAN GRADIENT BOOSTING WITH RMSE 0.1697073628206302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harpertran/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    }
   ],
   "source": [
    "(best_pred, rmse) = run_models(test, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7481e",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326cfb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission shape: (30002, 2), Best RMSE: 0.1697073628206302\n"
     ]
    }
   ],
   "source": [
    "def prepare_submission(test_df, predictions):\n",
    "    \"\"\"\n",
    "    Prepares the submission DataFrame.\n",
    "    :param test_df: DataFrame containing the test data.\n",
    "    :param predictions: Predictions to be included in the submission.\n",
    "    :return: Submission DataFrame.\n",
    "    \"\"\"\n",
    "    if not np.any(pd.isnull(predictions)):\n",
    "        submission = pd.DataFrame(test_df[['userId_movieId']])\n",
    "        submission['rating'] = predictions\n",
    "        print(f\"Submission shape: {submission.shape}, Best RMSE: {rmse}\")\n",
    "        return submission\n",
    "    else:\n",
    "        print(\"Predictions contain null values. Submission not created.\")\n",
    "        return None\n",
    "\n",
    "submission = prepare_submission(test, best_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82ad2b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId_movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>469_2124</td>\n",
       "      <td>0.663765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>439_3753</td>\n",
       "      <td>0.761249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>522_1682</td>\n",
       "      <td>0.938248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>429_1217</td>\n",
       "      <td>0.917605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71_1210</td>\n",
       "      <td>0.755397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>305_2599</td>\n",
       "      <td>0.696555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>22_2109</td>\n",
       "      <td>0.703634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>534_2947</td>\n",
       "      <td>0.810808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>558_4085</td>\n",
       "      <td>0.718926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30001</th>\n",
       "      <td>550_5693</td>\n",
       "      <td>0.644765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30002 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId_movieId    rating\n",
       "0           469_2124  0.663765\n",
       "1           439_3753  0.761249\n",
       "2           522_1682  0.938248\n",
       "3           429_1217  0.917605\n",
       "4            71_1210  0.755397\n",
       "...              ...       ...\n",
       "29997       305_2599  0.696555\n",
       "29998        22_2109  0.703634\n",
       "29999       534_2947  0.810808\n",
       "30000       558_4085  0.718926\n",
       "30001       550_5693  0.644765\n",
       "\n",
       "[30002 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 522.644346,
   "end_time": "2023-04-23T03:30:11.759556",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-23T03:21:29.115210",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
